\documentclass{article}

\usepackage{amsmath}

\newcommand{\meanx}{\ensuremath{\mu_{x}}}
\newcommand{\meanxsquared}{\ensuremath{\mu_{x}^{2}}}
\newcommand{\meany}{\ensuremath{\mu_{y}}}
\newcommand{\meanysquared}{\ensuremath{\mu_{y}^{2}}}
\newcommand{\meanxx}{\ensuremath{\mu_{xx}}}
\newcommand{\meanxy}{\ensuremath{\mu_{xy}}}
\newcommand{\meanyy}{\ensuremath{\mu_{yy}}}
\newcommand{\sxx}{\ensuremath{\sigma_{xx}}}
\newcommand{\sxy}{\ensuremath{\sigma_{xy}}}
\newcommand{\syy}{\ensuremath{\sigma_{yy}}}

\title{Simple Linear and Orthogonal Regression}
\author{J.S. Virzi}

\begin{document}
\maketitle

\section{Standard Linear Regression}

The cost function $S$

\begin{equation}
S \equiv \Sigma_{i=1}^{N} \left( y_{i} - m x_{i} - b \right)^{2}
\end{equation}

is minimized with respect to the parameters of the line, name $m$ and $b$.

\begin{equation}
-\frac{1}{2}\frac{\partial S}{\partial m} = \Sigma_{i=1}^{N} \left( y_{i} - m x_{i} - b \right) x_{i}
\end{equation}

\begin{equation}
-\frac{1}{2}\frac{\partial S}{\partial b} = \Sigma_{i=1}^{N} \left( y_{i} - m x_{i} - b \right)
\end{equation}

\begin{equation}
\frac{\partial S}{\partial m} = \frac{\partial S}{\partial b} = 0
\end{equation}

% \[
% M=
%   \begin{bmatrix}
%     1 & 2 & 3 & 4 & 5 \\
%     3 & 4 & 5 & 6 & 7
%   \end{bmatrix}
% \]

leads to the following system of equations

\begin{equation}
\left(
\begin{tabular}{c}
$\Sigma x_{i} y_{i}$ \\
$\Sigma y_{i}$
\end{tabular}
\right) = 
\left(
\begin{tabular}{cc}
$ \Sigma x_{i}^{2} $ & $\Sigma x_{i} $ \\
$ \Sigma x_{i} $ & $N$ \\
\end{tabular}
\right)
\left(
\begin{tabular}{c}
$m$ \\
$b$
\end{tabular}
\right)
\end{equation}

which can be reexpressed as

\begin{equation}
\left(
\begin{tabular}{c}
$\meanxy$ \\
$\meany$
\end{tabular}
\right) = 
\left(
\begin{tabular}{cc}
$ \meanxx $ & $ \meanx $ \\
$ \meanx $ & 1 \\
\end{tabular}
\right)
\left(
\begin{tabular}{c}
$m$ \\
$b$
\end{tabular}
\right)
\end{equation}

where the following definitions make the equations easier to work with

\begin{equation}
\meanx = \dfrac{1}{N} \Sigma_{i=1}^{N} x_{i}
\end{equation}

\begin{equation}
\meany = \dfrac{1}{N} \Sigma_{i=1}^{N} x_{y}
\end{equation}

\begin{equation}
\meanxx = \dfrac{1}{N} \Sigma_{i=1}^{N} x_{i}^{2}
\end{equation}

\begin{equation}
\meanxy = \dfrac{1}{N} \Sigma_{i=1}^{N} x_{i} y_{i}
\end{equation}

\begin{equation}
\meanyy = \frac{1}{N} \Sigma_{i=1}^{N} y_{i}^{2}
\end{equation}

and that leads to the following solution for $ \left( m, b \right)$

\begin{equation}
m = \frac{\sxy}{\sxx}
\end{equation}

\begin{equation}
b = \meany - m \meanx = \frac{\meany \meanxx - \meanx \meanxy}{\mu_{xx} - \mu_{x}^{2}}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%

The treatment of regression in the previous section assumes that $x_{i}$ are perfectly known, 
and that $y_{i}$ have equal errors associated with them.
In this section, we extend this treatment to the case where $x_{i}$ and $y_{i}$ have equal measurement errors.
Instead of drawing a ``vertical drop'' from each $y_{i}$,
it is more appropriate to consider a ``normal drop'' from each point $ \left( x_{i}, y_{i} \right) $ to the line.
The form of the cost function $S$ in EQUATION1 above is readily differentiated to solve standard linear regression.
Taking the same approach (modifying $S$) to account for the orthogonal distance between a point and a line
 is an inefficient starting point to solve this problem.
Instead, consider rotating each point to a reference frame where linear regression gives a slope $ m^{\prime} = 0 $.
In this frame, the vertical distance is also the orthogonal distance.
To obtain the desired result in our original frame, we rotate the horizontal line.

Start by rotating each point by an angle $ \theta $

% \begin{equation}
% \left(
% \begin{tabular}{cc}
% $ \cos \theta$ & $\sin \theta$ \\
%  - $\sin \theta$ & $\cos \theta$ \\
% \end{tabular}
% \right)
% \end{equation}

\begin{equation}
\left(
\begin{tabular}{c}
$x_{i}^{\prime}$ \\
$y_{i}^{\prime}$
\end{tabular}
\right) = 
\left(
\begin{tabular}{cc}
$ \cos \theta$ & $\sin \theta$ \\
 - $\sin \theta$ & $\cos \theta$ \\
\end{tabular}
\right)
\left(
\begin{tabular}{c}
$x_{i}$ \\
$y_{i}$
\end{tabular}
\right)
\end{equation}

The cost function $S^{\prime}$ in this frame is readily expressed

\begin{equation}
S^{\prime} \equiv \Sigma_{i=1}^{N} \left( \left( y_{i} \cos \theta - x_{i} \sin \theta \right) - m \left( x_{i} \cos \theta + y_{i} \sin \theta \right) - b \right)^{2}
\end{equation}

Differentiating EQUATION ABOVE

\begin{equation}
S^{\prime}_{i} \equiv y_{i} \cos \theta - x_{i} \sin \theta - m \left( x_{i} \cos \theta + y_{i} \sin \theta \right) - b
\end{equation}

\begin{equation}
-\frac{1}{2}\frac{\partial S^{\prime}}{\partial m} = \Sigma_{i=1}^{N} S^{\prime} \left( x_{i} \cos \theta + y_{i} \sin \theta \right)
\end{equation}

\begin{equation}
-\frac{1}{2}\frac{\partial S^{\prime}}{\partial b} = \Sigma_{i=1}^{N} S^{\prime} 
\end{equation}

\begin{equation}
\frac{\partial S}{\partial m} = \frac{\partial S}{\partial b} = 0
\end{equation}

leads to the following set of equations

\begin{eqnarray}
-\frac{1}{2}\frac{\partial S^{\prime}}{\partial m} & = & - \Sigma x_{i}^{2} \sin \theta \cos \theta - m \Sigma x_{i}^{2} \cos^{2} \theta 
+ \Sigma x_{i} y_{i} \left( \cos^{2} \theta  - \sin^{2} \theta \right) \\
& - & 2m \Sigma x_{i} y_{i} \cos \theta \sin \theta + \Sigma y_{i}^{2} \sin \theta \cos \theta - m \Sigma y_{i}^{2} \sin^{2} \theta \\
& - & b \left( x_{i} \cos \theta + y_{i} \sin \theta \right)
\end{eqnarray}

\begin{eqnarray}
-\frac{1}{2}\frac{\partial S^{\prime}}{\partial b} = 
\Sigma y_{i} \cos \theta - \Sigma x_{i} \sin \theta \\
- m \Sigma \left( x_{i} \cos \theta + y_{i} \sin \theta \right) - b
\end{eqnarray}


\begin{equation}
\sxx \equiv \meanxx - {\meanxsquared}
\end{equation}

\begin{equation}
\sxy \equiv \meanxy - \meanx \meany
\end{equation}

\begin{equation}
\syy \equiv \meanyy - \meanysquared
\end{equation}

\begin{equation}
\alpha \equiv \tan \left( 2 \theta \right) = \frac{2 \sxy}{\sxx - \syy}
\end{equation}

Using the half-angle formulae to express this

\begin{equation}
\tan \left( 2 \theta \right) = \frac{2 \tan \theta}{1 - \tan^{2} \theta}
\end{equation}

and since the slope $m = \tan \theta $, we arrive at the following formula for the slope of the line

\begin{equation}
m = \frac{1}{\alpha} \left( \sqrt{\alpha^{2} + 1} - 1\right)
\end{equation}

The extraneous solution is

\begin{equation}
m = - \frac{1}{\alpha} \left( \sqrt{\alpha^{2} + 1} + 1\right)
\end{equation}

Or simply

\begin{equation}
m = \frac{1}{2} \arctan \left( \frac{2 \sxy}{\sxx - \syy} \right)
\end{equation}

\end{document}